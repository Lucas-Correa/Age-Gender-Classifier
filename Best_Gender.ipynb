{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d726754a-9cb6-44d2-b37d-8824a0b53dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model,Sequential \n",
    "from tensorflow.keras import optimizers, regularizers \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed43f813-eec8-4ea4-84fd-773f2bb3c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_organize_files(): \n",
    "    #download files with images\n",
    "    #https://talhassner.github.io/home/projects/Adience/Adience-data.html\n",
    "    print('starting download')\n",
    "    BASE_URL = \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification\"\n",
    "    DOWNLOAD_FILES = {\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/aligned.tar.gz\": \"aligned.tar.gz\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_0_data.txt\": \"fold_0_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_1_data.txt\": \"fold_1_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_2_data.txt\": \"fold_2_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_3_data.txt\": \"fold_3_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_4_data.txt\": \"fold_4_data.txt\"\n",
    "                    }\n",
    "\n",
    "    for DOWNLOAD_FILE, FILE_NAME in DOWNLOAD_FILES.items():\n",
    "        if not os.path.exists(FILE_NAME):\n",
    "            with open(FILE_NAME, 'wb') as file:\n",
    "                r = requests.get(DOWNLOAD_FILE, auth = HTTPBasicAuth('adiencedb', 'adience'))\n",
    "                file.write(r.content)\n",
    "                print('downloaded {}'.format(FILE_NAME))\n",
    "    \n",
    "    print('unziping images...')\n",
    "    \n",
    "    if not os.path.exists('aligned'):\n",
    "        with tarfile.open('aligned.tar.gz') as file:\n",
    "            file.extractall()\n",
    "        \n",
    "    print('images unziped')\n",
    "    \n",
    "    #compile all files catalog\n",
    "    fold = pd.read_csv('fold_0_data.txt',sep='\\t')\n",
    "    fold.rename(columns={'user_id':'user_id'},inplace=True)\n",
    "    fold['fold'] = 0\n",
    "    for i in range(1,5):\n",
    "        temp = pd.read_csv('fold_'+str(i)+'_data.txt',sep='\\t')\n",
    "        temp['fold'] = i\n",
    "        fold = fold.append(temp,ignore_index=True)\n",
    "\n",
    "    fold.dropna(subset=['gender'],inplace=True)\n",
    "    fold = fold[['user_id','original_image','face_id','age','gender','fold']]\n",
    "     \n",
    "    train = fold.groupby('gender',as_index=False,group_keys=False).apply(lambda x: x.sample(frac=.6))\n",
    "    test = fold.drop(train.index).groupby('gender',as_index=False,group_keys=False).apply(lambda x: x.sample(frac=.4))\n",
    "    validation = fold.drop(train.index).drop(test.index)\n",
    "    \n",
    "    print('Train and test split done')\n",
    "    \n",
    "    #creates directories for training, test and validation files\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    train_faces = os.path.join(cwd,'train')\n",
    "    if not os.path.exists(train_faces):\n",
    "        os.mkdir(train_faces)\n",
    "\n",
    "    test_faces = os.path.join(cwd,'test')\n",
    "    if not os.path.exists(test_faces):\n",
    "        os.mkdir(test_faces)\n",
    "    \n",
    "    validation_faces = os.path.join(cwd,'validation')\n",
    "    if not os.path.exists(validation_faces):\n",
    "        os.mkdir(validation_faces)\n",
    "\n",
    "    #copy images to each directory\n",
    "    for index, row in train.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(train_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "\n",
    "    print('train images copied')\n",
    "\n",
    "    for index, row in test.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(test_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "    for index, row in validation.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(validation_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "\n",
    "    print('train images copied')\n",
    "\n",
    "    print('test images copied')\n",
    "    \n",
    "    print('validation images copied')\n",
    "    \n",
    "    return cwd,fold,train_faces,test_faces,validation_faces,train,test,validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f483a38a-b71d-4c8c-b315-8aaa6a06b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting download\n",
      "unziping images...\n",
      "images unziped\n",
      "Train and test split done\n",
      "train images copied\n",
      "train images copied\n",
      "test images copied\n",
      "validation images copied\n"
     ]
    }
   ],
   "source": [
    "cwd,fold,train_faces,test_faces,validation_faces,train_images_ids,test_images_ids,validation_images_ids = get_organize_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7416533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_gender_test2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0d4a7b2-ac6e-41f6-8915-e35481b5066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11154 validated image filenames belonging to 3 classes.\n",
      "Found 4462 validated image filenames belonging to 3 classes.\n",
      "Found 2975 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#vectorizing images \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    train_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=train_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical',\n",
    "                   # rotation_range = 40,\n",
    "                   # width_shift_range = 0.2,\n",
    "                   # height_shift_range = 0.2,\n",
    "                   # shear_range = 0.2,\n",
    "                   # zoom_range = 0.2,\n",
    "                   # horizontal_flip = True,\n",
    "                   # fill_mode = 'nearest'\n",
    "                    ) \n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "                    validation_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=validation_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical',\n",
    "                    )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                    test_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=test_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf03ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_acc', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22da8a6d-bd01-43de-9c6f-6b1bd62676f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 148, 148, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 74, 74, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 72, 72, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 70, 70, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 70, 70, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 35, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 33, 33, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 33, 33, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 3,658,275\n",
      "Trainable params: 3,655,395\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.3011 - acc: 0.5167\n",
      "Epoch 00001: val_acc improved from -inf to 0.51450, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 500s 5s/step - loss: 1.3011 - acc: 0.5167 - val_loss: 1.2326 - val_acc: 0.5145\n",
      "Epoch 2/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.1314 - acc: 0.5644\n",
      "Epoch 00002: val_acc did not improve from 0.51450\n",
      "110/110 [==============================] - 506s 5s/step - loss: 1.1314 - acc: 0.5644 - val_loss: 1.1725 - val_acc: 0.4460\n",
      "Epoch 3/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 1.0050 - acc: 0.6030\n",
      "Epoch 00003: val_acc improved from 0.51450 to 0.62125, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 506s 5s/step - loss: 1.0050 - acc: 0.6030 - val_loss: 1.1185 - val_acc: 0.6212\n",
      "Epoch 4/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.8880 - acc: 0.6614\n",
      "Epoch 00004: val_acc improved from 0.62125 to 0.66175, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 526s 5s/step - loss: 0.8880 - acc: 0.6614 - val_loss: 0.8922 - val_acc: 0.6618\n",
      "Epoch 5/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.8070 - acc: 0.6912\n",
      "Epoch 00005: val_acc improved from 0.66175 to 0.75200, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 505s 5s/step - loss: 0.8070 - acc: 0.6912 - val_loss: 0.7600 - val_acc: 0.7520\n",
      "Epoch 6/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7625 - acc: 0.7063\n",
      "Epoch 00006: val_acc improved from 0.75200 to 0.76825, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 495s 5s/step - loss: 0.7625 - acc: 0.7063 - val_loss: 0.7091 - val_acc: 0.7682\n",
      "Epoch 7/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7165 - acc: 0.7238\n",
      "Epoch 00007: val_acc did not improve from 0.76825\n",
      "110/110 [==============================] - 496s 5s/step - loss: 0.7165 - acc: 0.7238 - val_loss: 0.6850 - val_acc: 0.7555\n",
      "Epoch 8/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7672 - acc: 0.7124\n",
      "Epoch 00008: val_acc did not improve from 0.76825\n",
      "110/110 [==============================] - 496s 5s/step - loss: 0.7672 - acc: 0.7124 - val_loss: 0.8291 - val_acc: 0.6970\n",
      "Epoch 9/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7229 - acc: 0.7372\n",
      "Epoch 00009: val_acc did not improve from 0.76825\n",
      "110/110 [==============================] - 499s 5s/step - loss: 0.7229 - acc: 0.7372 - val_loss: 0.8069 - val_acc: 0.7585\n",
      "Epoch 10/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6643 - acc: 0.7571\n",
      "Epoch 00010: val_acc improved from 0.76825 to 0.79050, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 495s 5s/step - loss: 0.6643 - acc: 0.7571 - val_loss: 0.6526 - val_acc: 0.7905\n",
      "Epoch 11/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6262 - acc: 0.7708\n",
      "Epoch 00011: val_acc did not improve from 0.79050\n",
      "110/110 [==============================] - 495s 4s/step - loss: 0.6262 - acc: 0.7708 - val_loss: 0.6426 - val_acc: 0.7755\n",
      "Epoch 12/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5966 - acc: 0.7820\n",
      "Epoch 00012: val_acc improved from 0.79050 to 0.80450, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 496s 5s/step - loss: 0.5966 - acc: 0.7820 - val_loss: 0.6331 - val_acc: 0.8045\n",
      "Epoch 13/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5907 - acc: 0.7845\n",
      "Epoch 00013: val_acc did not improve from 0.80450\n",
      "110/110 [==============================] - 513s 5s/step - loss: 0.5907 - acc: 0.7845 - val_loss: 0.6776 - val_acc: 0.7843\n",
      "Epoch 14/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5946 - acc: 0.7836\n",
      "Epoch 00014: val_acc did not improve from 0.80450\n",
      "110/110 [==============================] - 503s 5s/step - loss: 0.5946 - acc: 0.7836 - val_loss: 0.6504 - val_acc: 0.7977\n",
      "Epoch 15/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5821 - acc: 0.7935\n",
      "Epoch 00015: val_acc improved from 0.80450 to 0.80775, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 519s 5s/step - loss: 0.5821 - acc: 0.7935 - val_loss: 0.6254 - val_acc: 0.8077\n",
      "Epoch 16/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5761 - acc: 0.7925\n",
      "Epoch 00016: val_acc did not improve from 0.80775\n",
      "110/110 [==============================] - 512s 5s/step - loss: 0.5761 - acc: 0.7925 - val_loss: 1.0228 - val_acc: 0.5767\n",
      "Epoch 17/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6233 - acc: 0.7825\n",
      "Epoch 00017: val_acc improved from 0.80775 to 0.80975, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 518s 5s/step - loss: 0.6233 - acc: 0.7825 - val_loss: 0.6840 - val_acc: 0.8098\n",
      "Epoch 18/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5589 - acc: 0.8085\n",
      "Epoch 00018: val_acc improved from 0.80975 to 0.81350, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 531s 5s/step - loss: 0.5589 - acc: 0.8085 - val_loss: 0.6442 - val_acc: 0.8135\n",
      "Epoch 19/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5531 - acc: 0.8088\n",
      "Epoch 00019: val_acc did not improve from 0.81350\n",
      "110/110 [==============================] - 532s 5s/step - loss: 0.5531 - acc: 0.8088 - val_loss: 0.7198 - val_acc: 0.7630\n",
      "Epoch 20/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5517 - acc: 0.8113\n",
      "Epoch 00020: val_acc did not improve from 0.81350\n",
      "110/110 [==============================] - 533s 5s/step - loss: 0.5517 - acc: 0.8113 - val_loss: 0.7042 - val_acc: 0.8075\n",
      "Epoch 21/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5294 - acc: 0.8230\n",
      "Epoch 00021: val_acc did not improve from 0.81350\n",
      "110/110 [==============================] - 541s 5s/step - loss: 0.5294 - acc: 0.8230 - val_loss: 0.6767 - val_acc: 0.8083\n",
      "Epoch 22/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5468 - acc: 0.8151\n",
      "Epoch 00022: val_acc improved from 0.81350 to 0.81975, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 553s 5s/step - loss: 0.5468 - acc: 0.8151 - val_loss: 0.6373 - val_acc: 0.8198\n",
      "Epoch 23/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5218 - acc: 0.8247\n",
      "Epoch 00023: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 512s 5s/step - loss: 0.5218 - acc: 0.8247 - val_loss: 0.7026 - val_acc: 0.8045\n",
      "Epoch 24/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5453 - acc: 0.8203\n",
      "Epoch 00024: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 502s 5s/step - loss: 0.5453 - acc: 0.8203 - val_loss: 0.7032 - val_acc: 0.7697\n",
      "Epoch 25/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5230 - acc: 0.8311\n",
      "Epoch 00025: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 514s 5s/step - loss: 0.5230 - acc: 0.8311 - val_loss: 0.7838 - val_acc: 0.8077\n",
      "Epoch 26/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5081 - acc: 0.8316\n",
      "Epoch 00026: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 529s 5s/step - loss: 0.5081 - acc: 0.8316 - val_loss: 0.6975 - val_acc: 0.7928\n",
      "Epoch 27/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5016 - acc: 0.8381\n",
      "Epoch 00027: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 541s 5s/step - loss: 0.5016 - acc: 0.8381 - val_loss: 0.7483 - val_acc: 0.8190\n",
      "Epoch 28/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5453 - acc: 0.8251\n",
      "Epoch 00028: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 534s 5s/step - loss: 0.5453 - acc: 0.8251 - val_loss: 0.7451 - val_acc: 0.8033\n",
      "Epoch 29/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5411 - acc: 0.8339\n",
      "Epoch 00029: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 522s 5s/step - loss: 0.5411 - acc: 0.8339 - val_loss: 0.7405 - val_acc: 0.7790\n",
      "Epoch 30/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5390 - acc: 0.8308\n",
      "Epoch 00030: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 510s 5s/step - loss: 0.5390 - acc: 0.8308 - val_loss: 0.9321 - val_acc: 0.7410\n",
      "Epoch 31/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5752 - acc: 0.8279\n",
      "Epoch 00031: val_acc did not improve from 0.81975\n",
      "110/110 [==============================] - 525s 5s/step - loss: 0.5752 - acc: 0.8279 - val_loss: 0.7084 - val_acc: 0.8198\n",
      "Epoch 32/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5439 - acc: 0.8362\n",
      "Epoch 00032: val_acc improved from 0.81975 to 0.83025, saving model to best_model0.hdf5\n",
      "110/110 [==============================] - 517s 5s/step - loss: 0.5439 - acc: 0.8362 - val_loss: 0.7479 - val_acc: 0.8303\n",
      "Epoch 33/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5741 - acc: 0.8238\n",
      "Epoch 00033: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 515s 5s/step - loss: 0.5741 - acc: 0.8238 - val_loss: 0.8689 - val_acc: 0.7280\n",
      "Epoch 34/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6070 - acc: 0.8272\n",
      "Epoch 00034: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 520s 5s/step - loss: 0.6070 - acc: 0.8272 - val_loss: 0.8670 - val_acc: 0.7845\n",
      "Epoch 35/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6155 - acc: 0.8255\n",
      "Epoch 00035: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 518s 5s/step - loss: 0.6155 - acc: 0.8255 - val_loss: 0.7474 - val_acc: 0.8140\n",
      "Epoch 36/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5444 - acc: 0.8483\n",
      "Epoch 00036: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 526s 5s/step - loss: 0.5444 - acc: 0.8483 - val_loss: 0.7681 - val_acc: 0.8267\n",
      "Epoch 37/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4941 - acc: 0.8586\n",
      "Epoch 00037: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 529s 5s/step - loss: 0.4941 - acc: 0.8586 - val_loss: 0.7559 - val_acc: 0.8202\n",
      "Epoch 38/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4915 - acc: 0.8524\n",
      "Epoch 00038: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 529s 5s/step - loss: 0.4915 - acc: 0.8524 - val_loss: 0.7741 - val_acc: 0.7640\n",
      "Epoch 39/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6713 - acc: 0.7994\n",
      "Epoch 00039: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 514s 5s/step - loss: 0.6713 - acc: 0.7994 - val_loss: 0.7889 - val_acc: 0.7952\n",
      "Epoch 40/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5866 - acc: 0.8336\n",
      "Epoch 00040: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 518s 5s/step - loss: 0.5866 - acc: 0.8336 - val_loss: 0.8081 - val_acc: 0.7703\n",
      "Epoch 41/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5499 - acc: 0.8438\n",
      "Epoch 00041: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 516s 5s/step - loss: 0.5499 - acc: 0.8438 - val_loss: 0.7894 - val_acc: 0.8012\n",
      "Epoch 42/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5964 - acc: 0.8285\n",
      "Epoch 00042: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 526s 5s/step - loss: 0.5964 - acc: 0.8285 - val_loss: 0.7720 - val_acc: 0.7935\n",
      "Epoch 43/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5861 - acc: 0.8315\n",
      "Epoch 00043: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 518s 5s/step - loss: 0.5861 - acc: 0.8315 - val_loss: 0.8172 - val_acc: 0.7875\n",
      "Epoch 44/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5463 - acc: 0.8467\n",
      "Epoch 00044: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 517s 5s/step - loss: 0.5463 - acc: 0.8467 - val_loss: 0.7683 - val_acc: 0.8080\n",
      "Epoch 45/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 0s - loss: 0.4966 - acc: 0.8610\n",
      "Epoch 00045: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 518s 5s/step - loss: 0.4966 - acc: 0.8610 - val_loss: 0.8184 - val_acc: 0.7750\n",
      "Epoch 46/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4906 - acc: 0.8580\n",
      "Epoch 00046: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 521s 5s/step - loss: 0.4906 - acc: 0.8580 - val_loss: 0.7753 - val_acc: 0.8192\n",
      "Epoch 47/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4752 - acc: 0.8561\n",
      "Epoch 00047: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 520s 5s/step - loss: 0.4752 - acc: 0.8561 - val_loss: 0.8279 - val_acc: 0.8175\n",
      "Epoch 48/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4446 - acc: 0.8623\n",
      "Epoch 00048: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 519s 5s/step - loss: 0.4446 - acc: 0.8623 - val_loss: 0.7023 - val_acc: 0.8110\n",
      "Epoch 49/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4944 - acc: 0.8480\n",
      "Epoch 00049: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 521s 5s/step - loss: 0.4944 - acc: 0.8480 - val_loss: 0.7677 - val_acc: 0.7960\n",
      "Epoch 50/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6186 - acc: 0.8159\n",
      "Epoch 00050: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 523s 5s/step - loss: 0.6186 - acc: 0.8159 - val_loss: 0.8794 - val_acc: 0.8008\n",
      "Epoch 51/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5488 - acc: 0.8497\n",
      "Epoch 00051: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 521s 5s/step - loss: 0.5488 - acc: 0.8497 - val_loss: 0.7652 - val_acc: 0.8155\n",
      "Epoch 52/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4969 - acc: 0.8577\n",
      "Epoch 00052: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 528s 5s/step - loss: 0.4969 - acc: 0.8577 - val_loss: 0.8066 - val_acc: 0.8085\n",
      "Epoch 53/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4485 - acc: 0.8669\n",
      "Epoch 00053: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 552s 5s/step - loss: 0.4485 - acc: 0.8669 - val_loss: 0.7609 - val_acc: 0.8275\n",
      "Epoch 54/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4479 - acc: 0.8638\n",
      "Epoch 00054: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 517s 5s/step - loss: 0.4479 - acc: 0.8638 - val_loss: 0.7471 - val_acc: 0.8253\n",
      "Epoch 55/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4503 - acc: 0.8610\n",
      "Epoch 00055: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 513s 5s/step - loss: 0.4503 - acc: 0.8610 - val_loss: 0.8486 - val_acc: 0.7753\n",
      "Epoch 56/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4616 - acc: 0.8623\n",
      "Epoch 00056: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 521s 5s/step - loss: 0.4616 - acc: 0.8623 - val_loss: 0.7802 - val_acc: 0.8142\n",
      "Epoch 57/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4385 - acc: 0.8652\n",
      "Epoch 00057: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 519s 5s/step - loss: 0.4385 - acc: 0.8652 - val_loss: 0.7856 - val_acc: 0.8067\n",
      "Epoch 58/300\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4180 - acc: 0.8724\n",
      "Epoch 00058: val_acc did not improve from 0.83025\n",
      "110/110 [==============================] - 543s 5s/step - loss: 0.4180 - acc: 0.8724 - val_loss: 0.8175 - val_acc: 0.8183\n",
      "Epoch 59/300\n",
      " 46/110 [===========>..................] - ETA: 4:40 - loss: 0.4112 - acc: 0.8731"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-556da8d55003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_gender_test2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model.fit_generator(train_generator,\n\u001b[0m\u001b[0;32m      4\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \"\"\"\n\u001b[0;32m   1814\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1816\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training model\n",
    "model = Model_gender_test2()\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=110,\n",
    "                              epochs=300,\n",
    "                              validation_data = validation_generator,\n",
    "                              validation_steps=40,\n",
    "                              callbacks=[checkpoint])\n",
    "model.save('face_gender_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93543f3-b265-4996-9dd1-9bb992a65077",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_generator(test_generator, (9296 / 20)) # ceil(num_of_test_samples / batch_size)\n",
    "target_names = ['Men', 'Woman','Baby']\n",
    "print(classification_report(test_generator.classes[:Y_pred.shape[0]],\n",
    "                            list(np.argmax(Y_pred,axis=1)), \n",
    "                            target_names=target_names))\n",
    "\n",
    "\n",
    "acc_train = history.history['acc']\n",
    "acc_val = history.history['val_acc']\n",
    "epochs = range(1,51)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, acc_val, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,51)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
