{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d726754a-9cb6-44d2-b37d-8824a0b53dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Model,Sequential \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed43f813-eec8-4ea4-84fd-773f2bb3c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download files with images\n",
    "#https://talhassner.github.io/home/projects/Adience/Adience-data.html\n",
    "BASE_URL = \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification\"\n",
    "DOWNLOAD_FILES = {\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/aligned.tar.gz\": \"aligned.tar.gz\",\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_0_data.txt\": \"fold_0_data.txt\",\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_1_data.txt\": \"fold_1_data.txt\",\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_2_data.txt\": \"fold_2_data.txt\",\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_3_data.txt\": \"fold_3_data.txt\",\n",
    "          \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_4_data.txt\": \"fold_4_data.txt\"\n",
    "                }\n",
    "\n",
    "for DOWNLOAD_FILE, FILE_NAME in DOWNLOAD_FILES.items():\n",
    "    with open(FILE_NAME, 'wb') as file:\n",
    "        r = requests.get(DOWNLOAD_FILE, auth = HTTPBasicAuth('adiencedb', 'adience'))\n",
    "        file.write(r.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83168ebe-ac46-49fb-b653-dae2677a547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile all files catalog\n",
    "fold = pd.read_csv('fold_0_data.txt',sep='\\t')\n",
    "fold.rename(columns={' user_id':'user_id'},inplace=True)\n",
    "fold['fold'] = 0\n",
    "for i in range(1,5):\n",
    "    temp = pd.read_csv('fold_'+str(i)+'_data.txt',sep='\\t')\n",
    "    temp['fold'] = i\n",
    "    fold = fold.append(temp,ignore_index=True)\n",
    "    \n",
    "fold.dropna(subset=['gender'],inplace=True)\n",
    "fold = fold[['user_id','original_image','face_id','age','gender','fold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6be9452-5c48-439b-92f1-d0209b7e7425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399646885_67c7d20df9_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10424815813_e94629b1ec_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>11816644924_075c3d8d59_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19341</th>\n",
       "      <td>101515718@N03</td>\n",
       "      <td>10587826073_6663f5b654_o.jpg</td>\n",
       "      <td>2280</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19342</th>\n",
       "      <td>101515718@N03</td>\n",
       "      <td>10587571495_a61785cd06_o.jpg</td>\n",
       "      <td>2278</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19343</th>\n",
       "      <td>101515718@N03</td>\n",
       "      <td>10587571495_a61785cd06_o.jpg</td>\n",
       "      <td>2279</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19344</th>\n",
       "      <td>50458575@N08</td>\n",
       "      <td>9426695459_9e8b347604_o.jpg</td>\n",
       "      <td>2281</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19345</th>\n",
       "      <td>50458575@N08</td>\n",
       "      <td>9429464468_1bfc39ecfb_o.jpg</td>\n",
       "      <td>2281</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18591 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id                original_image  face_id       age gender  \\\n",
       "0       30601258@N03  10399646885_67c7d20df9_o.jpg        1  (25, 32)      f   \n",
       "1       30601258@N03  10424815813_e94629b1ec_o.jpg        2  (25, 32)      m   \n",
       "2       30601258@N03  10437979845_5985be4b26_o.jpg        1  (25, 32)      f   \n",
       "3       30601258@N03  10437979845_5985be4b26_o.jpg        3  (25, 32)      m   \n",
       "4       30601258@N03  11816644924_075c3d8d59_o.jpg        2  (25, 32)      m   \n",
       "...              ...                           ...      ...       ...    ...   \n",
       "19341  101515718@N03  10587826073_6663f5b654_o.jpg     2280  (25, 32)      f   \n",
       "19342  101515718@N03  10587571495_a61785cd06_o.jpg     2278  (25, 32)      m   \n",
       "19343  101515718@N03  10587571495_a61785cd06_o.jpg     2279  (25, 32)      f   \n",
       "19344   50458575@N08   9426695459_9e8b347604_o.jpg     2281  (25, 32)      f   \n",
       "19345   50458575@N08   9429464468_1bfc39ecfb_o.jpg     2281  (25, 32)      f   \n",
       "\n",
       "       fold  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "19341     4  \n",
       "19342     4  \n",
       "19343     4  \n",
       "19344     4  \n",
       "19345     4  \n",
       "\n",
       "[18591 rows x 6 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d4c36c0-1f5b-433e-b158-a13af966b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates directories for training, test and validation files\n",
    "cwd = os.getcwd()\n",
    "train_faces = os.path.join(cwd,'train')\n",
    "os.mkdir(train_faces)\n",
    "test_faces = os.path.join(cwd,'test')\n",
    "os.mkdir(test_faces)\n",
    "validation_faces = os.path.join(cwd,'validation')\n",
    "os.mkdir(validation_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "757857d5-fbcc-4104-931d-c1c993102bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagens de treino copiadas\n",
      "imagens de test copiadas\n",
      "imagens de validation copiadas\n"
     ]
    }
   ],
   "source": [
    "#copy images to each directory\n",
    "for index, row in fold.loc[fold.fold==0].iterrows():\n",
    "    src = os.path.join(cwd,'faces',row['user_id'],'coarse_tilt_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "    dst = os.path.join(train_faces,row['original_image'])\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "print('imagens de treino copiadas')\n",
    "\n",
    "for index, row in fold.loc[fold.fold==1].iterrows():\n",
    "    src = os.path.join(cwd,'faces',row['user_id'],'coarse_tilt_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "    dst = os.path.join(test_faces,row['original_image'])\n",
    "    shutil.copyfile(src,dst)\n",
    "\n",
    "print('imagens de test copiadas')\n",
    "\n",
    "for index, row in fold.loc[fold.fold==2].iterrows():\n",
    "    src = os.path.join(cwd,'faces',row['user_id'],'coarse_tilt_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "    dst = os.path.join(validation_faces,row['original_image'])\n",
    "    shutil.copyfile(src,dst)\n",
    "\n",
    "print('imagens de validation copiadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc9e2901-ffbd-4511-8536-02044e1230c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 3, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimazer = optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0d4a7b2-ac6e-41f6-8915-e35481b5066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4431 validated image filenames belonging to 3 classes.\n",
      "Found 3692 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 14160 invalid image filename(s) in x_col=\"original_image\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "/opt/anaconda3/envs/TensorFlow/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 14899 invalid image filename(s) in x_col=\"original_image\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "#vectorizing images \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    fold,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=train_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=20,\n",
    "                    class_mode='categorical'\n",
    "                )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                    fold,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=test_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=20,\n",
    "                    class_mode='categorical'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22da8a6d-bd01-43de-9c6f-6b1bd62676f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 2.6525 - acc: 0.4960 - val_loss: 0.8266 - val_acc: 0.5767\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.8898 - acc: 0.5665 - val_loss: 0.8480 - val_acc: 0.5467\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.8022 - acc: 0.6178 - val_loss: 0.8046 - val_acc: 0.5733\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.7783 - acc: 0.6215 - val_loss: 0.7970 - val_acc: 0.5600\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.7067 - acc: 0.6735 - val_loss: 0.8677 - val_acc: 0.5233\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.6529 - acc: 0.6986 - val_loss: 0.7788 - val_acc: 0.5833\n",
      "Epoch 7/15\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.6286 - acc: 0.7035 - val_loss: 0.8852 - val_acc: 0.5333\n",
      "Epoch 8/15\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.5674 - acc: 0.7375 - val_loss: 0.8153 - val_acc: 0.5600\n",
      "Epoch 9/15\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.5750 - acc: 0.7303 - val_loss: 0.9578 - val_acc: 0.6000\n",
      "Epoch 10/15\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.4888 - acc: 0.7775 - val_loss: 0.8547 - val_acc: 0.5633\n",
      "Epoch 11/15\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.5386 - acc: 0.7520 - val_loss: 0.9328 - val_acc: 0.5533\n",
      "Epoch 12/15\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.4674 - acc: 0.7755 - val_loss: 1.0560 - val_acc: 0.5167\n",
      "Epoch 13/15\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.4703 - acc: 0.7891 - val_loss: 0.9407 - val_acc: 0.5367\n",
      "Epoch 14/15\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.4345 - acc: 0.8036 - val_loss: 1.1111 - val_acc: 0.5933\n",
      "Epoch 15/15\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.4495 - acc: 0.7985 - val_loss: 0.9417 - val_acc: 0.5700\n"
     ]
    }
   ],
   "source": [
    "#training model\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=100,\n",
    "                              epochs=15,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=15)\n",
    "model.save('face_gender_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93543f3-b265-4996-9dd1-9bb992a65077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have the last version of tensorflow, the predict_generator is deprecated.\n",
    "# you should use the predict method.\n",
    "# if you do not have the last version, you must use predict_generator\n",
    "Y_pred = model.predict_generator(test_set, 63) # ceil(num_of_test_samples / batch_size)\n",
    "Y_pred = (Y_pred>0.5)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_set.classes, Y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Cats', 'Dogs']\n",
    "print(classification_report(test_set.classes, Y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "loss_train = hist.history['accuracy']\n",
    "loss_val = hist.history['val_accuracy']\n",
    "epochs = range(1,6)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss_train = hist.history['loss']\n",
    "loss_val = hist.history['val_loss']\n",
    "epochs = range(1,6)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605964c-4fbb-4e26-abfc-bc5bb361ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucas and Francisco \n",
    "\n",
    "decide the metric (acc,f1,...?)\n",
    "finish the viz of this metric\n",
    "create the model of age classification\n",
    "organize better the images into our splits\n",
    "benchmark between papers\n",
    "post in git \n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5fb77-fdb7-41e0-aeb7-77db076d4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bruna and Isha\n",
    "\n",
    "Topic: how children change you vocation?\n",
    "get deeper in the EDA\n",
    "What we are going to show\n",
    "How are we going to show\n",
    "How to make interactive \n",
    "post in git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
