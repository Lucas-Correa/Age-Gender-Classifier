{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d726754a-9cb6-44d2-b37d-8824a0b53dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model,Sequential \n",
    "from tensorflow.keras import optimizers, regularizers \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed43f813-eec8-4ea4-84fd-773f2bb3c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_organize_files(): \n",
    "    #download files with images\n",
    "    #https://talhassner.github.io/home/projects/Adience/Adience-data.html\n",
    "    print('starting download')\n",
    "    BASE_URL = \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification\"\n",
    "    DOWNLOAD_FILES = {\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/aligned.tar.gz\": \"aligned.tar.gz\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_0_data.txt\": \"fold_0_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_1_data.txt\": \"fold_1_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_2_data.txt\": \"fold_2_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_3_data.txt\": \"fold_3_data.txt\",\n",
    "              \"http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/fold_4_data.txt\": \"fold_4_data.txt\"\n",
    "                    }\n",
    "\n",
    "    for DOWNLOAD_FILE, FILE_NAME in DOWNLOAD_FILES.items():\n",
    "        if not os.path.exists(FILE_NAME):\n",
    "            with open(FILE_NAME, 'wb') as file:\n",
    "                r = requests.get(DOWNLOAD_FILE, auth = HTTPBasicAuth('adiencedb', 'adience'))\n",
    "                file.write(r.content)\n",
    "                print('downloaded {}'.format(FILE_NAME))\n",
    "    \n",
    "    print('unziping images...')\n",
    "    \n",
    "    if not os.path.exists('aligned'):\n",
    "        with tarfile.open('aligned.tar.gz') as file:\n",
    "            file.extractall()\n",
    "        \n",
    "    print('images unziped')\n",
    "    \n",
    "    #compile all files catalog\n",
    "    fold = pd.read_csv('fold_0_data.txt',sep='\\t')\n",
    "    fold.rename(columns={'user_id':'user_id'},inplace=True)\n",
    "    fold['fold'] = 0\n",
    "    for i in range(1,5):\n",
    "        temp = pd.read_csv('fold_'+str(i)+'_data.txt',sep='\\t')\n",
    "        temp['fold'] = i\n",
    "        fold = fold.append(temp,ignore_index=True)\n",
    "\n",
    "    fold.dropna(subset=['gender'],inplace=True)\n",
    "    fold = fold[['user_id','original_image','face_id','age','gender','fold']]\n",
    "     \n",
    "    train = fold.groupby('gender',as_index=False,group_keys=False).apply(lambda x: x.sample(frac=.6))\n",
    "    test = fold.drop(train.index).groupby('gender',as_index=False,group_keys=False).apply(lambda x: x.sample(frac=.4))\n",
    "    validation = fold.drop(train.index).drop(test.index)\n",
    "    \n",
    "    print('Train and test split done')\n",
    "    \n",
    "    #creates directories for training, test and validation files\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    train_faces = os.path.join(cwd,'train')\n",
    "    if not os.path.exists(train_faces):\n",
    "        os.mkdir(train_faces)\n",
    "\n",
    "    test_faces = os.path.join(cwd,'test')\n",
    "    if not os.path.exists(test_faces):\n",
    "        os.mkdir(test_faces)\n",
    "    \n",
    "    validation_faces = os.path.join(cwd,'validation')\n",
    "    if not os.path.exists(validation_faces):\n",
    "        os.mkdir(validation_faces)\n",
    "\n",
    "    #copy images to each directory\n",
    "    for index, row in train.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(train_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "\n",
    "    print('train images copied')\n",
    "\n",
    "    for index, row in test.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(test_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "    for index, row in validation.iterrows():\n",
    "        src = os.path.join(cwd,'aligned',row['user_id'],'landmark_aligned_face.'+str(row['face_id'])+'.'+row['original_image'])\n",
    "        dst = os.path.join(validation_faces,row['original_image'])\n",
    "        shutil.copyfile(src,dst)\n",
    "\n",
    "    print('train images copied')\n",
    "\n",
    "    print('test images copied')\n",
    "    \n",
    "    print('validation images copied')\n",
    "    \n",
    "    return cwd,fold,train_faces,test_faces,validation_faces,train,test,validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f483a38a-b71d-4c8c-b315-8aaa6a06b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting download\n",
      "unziping images...\n",
      "images unziped\n",
      "Train and test split done\n",
      "train images copied\n",
      "train images copied\n",
      "test images copied\n",
      "validation images copied\n"
     ]
    }
   ],
   "source": [
    "cwd,fold,train_faces,test_faces,validation_faces,train_images_ids,test_images_ids,validation_images_ids = get_organize_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9e2901-ffbd-4511-8536-02044e1230c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(32, (1, 1), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 192, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c2d4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_so_far():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(128, (7, 7), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(256, (7, 7), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 16, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252a164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_gender():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu'), kernel_regularizer=regularizers.L2(l2= 0.001))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation = 'relu'), kernel_regularizer=regularizers.L2(l2= 0.001))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation = 'relu'), kernel_regularizer=regularizers.L2(l2= 0.001))\n",
    "    model.add(BatchNormalization())\n",
    "     \n",
    "    model.add(Conv2D(512, (5, 5), activation = 'relu'), kernel_regularizer=regularizers.L2(l2= 0.001))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f55e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_gender_test():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7416533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_gender_test2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer=regularizers.L2(l2= 0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7534b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_gender2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (5, 5), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (7, 7), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(256, (7, 7), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units = 256, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(units = 16, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d4a7b2-ac6e-41f6-8915-e35481b5066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11154 validated image filenames belonging to 3 classes.\n",
      "Found 4462 validated image filenames belonging to 3 classes.\n",
      "Found 2975 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#vectorizing images \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    train_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=train_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=30,\n",
    "                    class_mode='categorical',\n",
    "                   # rotation_range = 40,\n",
    "                   # width_shift_range = 0.2,\n",
    "                   # height_shift_range = 0.2,\n",
    "                   # shear_range = 0.2,\n",
    "                   # zoom_range = 0.2,\n",
    "                   # horizontal_flip = True,\n",
    "                   # fill_mode = 'nearest'\n",
    "                    ) \n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "                    validation_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=validation_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=30,\n",
    "                    class_mode='categorical',\n",
    "                    )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                    test_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='gender',\n",
    "                    directory=test_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf03ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_acc', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22da8a6d-bd01-43de-9c6f-6b1bd62676f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 148, 148, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 74, 74, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 72, 72, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 70, 70, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 70, 70, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 35, 35, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 33, 33, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 33, 33, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_105 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 3,658,275\n",
      "Trainable params: 3,655,395\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.7739 - acc: 0.2760\n",
      "Epoch 00001: val_acc improved from -inf to 0.50267, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 83s 2s/step - loss: 1.7739 - acc: 0.2760 - val_loss: 1.5280 - val_acc: 0.5027\n",
      "Epoch 2/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.3601 - acc: 0.4920\n",
      "Epoch 00002: val_acc did not improve from 0.50267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.3601 - acc: 0.4920 - val_loss: 1.3558 - val_acc: 0.5027\n",
      "Epoch 3/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.3061 - acc: 0.5040\n",
      "Epoch 00003: val_acc improved from 0.50267 to 0.51400, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 95s 2s/step - loss: 1.3061 - acc: 0.5040 - val_loss: 1.2795 - val_acc: 0.5140\n",
      "Epoch 4/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2412 - acc: 0.5140\n",
      "Epoch 00004: val_acc improved from 0.51400 to 0.51733, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.2412 - acc: 0.5140 - val_loss: 1.2163 - val_acc: 0.5173\n",
      "Epoch 5/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2271 - acc: 0.5127\n",
      "Epoch 00005: val_acc did not improve from 0.51733\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.2271 - acc: 0.5127 - val_loss: 1.2209 - val_acc: 0.5007\n",
      "Epoch 6/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1912 - acc: 0.5380\n",
      "Epoch 00006: val_acc improved from 0.51733 to 0.52533, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.1912 - acc: 0.5380 - val_loss: 1.1765 - val_acc: 0.5253\n",
      "Epoch 7/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1534 - acc: 0.5487\n",
      "Epoch 00007: val_acc did not improve from 0.52533\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.1534 - acc: 0.5487 - val_loss: 1.1693 - val_acc: 0.4947\n",
      "Epoch 8/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1499 - acc: 0.5320\n",
      "Epoch 00008: val_acc improved from 0.52533 to 0.55600, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.1499 - acc: 0.5320 - val_loss: 1.1130 - val_acc: 0.5560\n",
      "Epoch 9/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0955 - acc: 0.5460\n",
      "Epoch 00009: val_acc improved from 0.55600 to 0.56267, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0955 - acc: 0.5460 - val_loss: 1.1209 - val_acc: 0.5627\n",
      "Epoch 10/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0889 - acc: 0.5680\n",
      "Epoch 00010: val_acc did not improve from 0.56267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0889 - acc: 0.5680 - val_loss: 1.1790 - val_acc: 0.5520\n",
      "Epoch 11/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0803 - acc: 0.5340\n",
      "Epoch 00011: val_acc did not improve from 0.56267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0803 - acc: 0.5340 - val_loss: 1.1715 - val_acc: 0.5267\n",
      "Epoch 12/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0867 - acc: 0.5447\n",
      "Epoch 00012: val_acc improved from 0.56267 to 0.58667, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0867 - acc: 0.5447 - val_loss: 1.0377 - val_acc: 0.5867\n",
      "Epoch 13/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0456 - acc: 0.5480\n",
      "Epoch 00013: val_acc improved from 0.58667 to 0.58733, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0456 - acc: 0.5480 - val_loss: 1.0389 - val_acc: 0.5873\n",
      "Epoch 14/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0224 - acc: 0.5767\n",
      "Epoch 00014: val_acc improved from 0.58733 to 0.59467, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.0224 - acc: 0.5767 - val_loss: 0.9861 - val_acc: 0.5947\n",
      "Epoch 15/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9782 - acc: 0.5803\n",
      "Epoch 00015: val_acc did not improve from 0.59467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9782 - acc: 0.5803 - val_loss: 0.9573 - val_acc: 0.5940\n",
      "Epoch 16/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9414 - acc: 0.5933\n",
      "Epoch 00016: val_acc improved from 0.59467 to 0.60467, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.9414 - acc: 0.5933 - val_loss: 0.9697 - val_acc: 0.6047\n",
      "Epoch 17/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9807 - acc: 0.5813\n",
      "Epoch 00017: val_acc did not improve from 0.60467\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.9807 - acc: 0.5813 - val_loss: 0.9560 - val_acc: 0.6040\n",
      "Epoch 18/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9370 - acc: 0.5847\n",
      "Epoch 00018: val_acc did not improve from 0.60467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9370 - acc: 0.5847 - val_loss: 0.9933 - val_acc: 0.6013\n",
      "Epoch 19/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9139 - acc: 0.6100\n",
      "Epoch 00019: val_acc improved from 0.60467 to 0.66467, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9139 - acc: 0.6100 - val_loss: 0.8786 - val_acc: 0.6647\n",
      "Epoch 20/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9214 - acc: 0.6100\n",
      "Epoch 00020: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9214 - acc: 0.6100 - val_loss: 0.9507 - val_acc: 0.5980\n",
      "Epoch 21/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9066 - acc: 0.5827\n",
      "Epoch 00021: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.9066 - acc: 0.5827 - val_loss: 0.9878 - val_acc: 0.6053\n",
      "Epoch 22/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8743 - acc: 0.6027\n",
      "Epoch 00022: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8743 - acc: 0.6027 - val_loss: 0.8634 - val_acc: 0.6320\n",
      "Epoch 23/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8844 - acc: 0.6080\n",
      "Epoch 00023: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8844 - acc: 0.6080 - val_loss: 0.8829 - val_acc: 0.6327\n",
      "Epoch 24/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8716 - acc: 0.6127\n",
      "Epoch 00024: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8716 - acc: 0.6127 - val_loss: 0.8946 - val_acc: 0.6100\n",
      "Epoch 25/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8297 - acc: 0.6320\n",
      "Epoch 00025: val_acc did not improve from 0.66467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8297 - acc: 0.6320 - val_loss: 0.8917 - val_acc: 0.6180\n",
      "Epoch 26/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8450 - acc: 0.6200\n",
      "Epoch 00026: val_acc improved from 0.66467 to 0.69467, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.8450 - acc: 0.6200 - val_loss: 0.8136 - val_acc: 0.6947\n",
      "Epoch 27/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8349 - acc: 0.6120\n",
      "Epoch 00027: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8349 - acc: 0.6120 - val_loss: 0.8638 - val_acc: 0.6540\n",
      "Epoch 28/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8486 - acc: 0.6207\n",
      "Epoch 00028: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8486 - acc: 0.6207 - val_loss: 0.8980 - val_acc: 0.6133\n",
      "Epoch 29/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8387 - acc: 0.6253\n",
      "Epoch 00029: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8387 - acc: 0.6253 - val_loss: 0.8069 - val_acc: 0.6600\n",
      "Epoch 30/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8482 - acc: 0.6120\n",
      "Epoch 00030: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8482 - acc: 0.6120 - val_loss: 0.8014 - val_acc: 0.6560\n",
      "Epoch 31/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8218 - acc: 0.6200\n",
      "Epoch 00031: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8218 - acc: 0.6200 - val_loss: 0.9774 - val_acc: 0.6127\n",
      "Epoch 32/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8010 - acc: 0.6453\n",
      "Epoch 00032: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8010 - acc: 0.6453 - val_loss: 0.8327 - val_acc: 0.6193\n",
      "Epoch 33/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8362 - acc: 0.6287\n",
      "Epoch 00033: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8362 - acc: 0.6287 - val_loss: 0.8757 - val_acc: 0.6100\n",
      "Epoch 34/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8703 - acc: 0.5957\n",
      "Epoch 00034: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8703 - acc: 0.5957 - val_loss: 0.8962 - val_acc: 0.6200\n",
      "Epoch 35/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9047 - acc: 0.6393\n",
      "Epoch 00035: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9047 - acc: 0.6393 - val_loss: 0.9147 - val_acc: 0.6180\n",
      "Epoch 36/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8645 - acc: 0.6373\n",
      "Epoch 00036: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8645 - acc: 0.6373 - val_loss: 0.9502 - val_acc: 0.5987\n",
      "Epoch 37/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8700 - acc: 0.6131\n",
      "Epoch 00037: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8700 - acc: 0.6131 - val_loss: 0.8508 - val_acc: 0.6253\n",
      "Epoch 38/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8278 - acc: 0.6447\n",
      "Epoch 00038: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8278 - acc: 0.6447 - val_loss: 0.9143 - val_acc: 0.6300\n",
      "Epoch 39/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8093 - acc: 0.6460\n",
      "Epoch 00039: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8093 - acc: 0.6460 - val_loss: 0.7931 - val_acc: 0.6600\n",
      "Epoch 40/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8161 - acc: 0.6480\n",
      "Epoch 00040: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8161 - acc: 0.6480 - val_loss: 0.8242 - val_acc: 0.6433\n",
      "Epoch 41/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8166 - acc: 0.6419\n",
      "Epoch 00041: val_acc did not improve from 0.69467\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8166 - acc: 0.6419 - val_loss: 0.8770 - val_acc: 0.6327\n",
      "Epoch 42/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8057 - acc: 0.6460\n",
      "Epoch 00042: val_acc improved from 0.69467 to 0.69600, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8057 - acc: 0.6460 - val_loss: 0.7535 - val_acc: 0.6960\n",
      "Epoch 43/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7988 - acc: 0.6567\n",
      "Epoch 00043: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7988 - acc: 0.6567 - val_loss: 0.8095 - val_acc: 0.6127\n",
      "Epoch 44/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7752 - acc: 0.6753\n",
      "Epoch 00044: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7752 - acc: 0.6753 - val_loss: 0.8597 - val_acc: 0.6100\n",
      "Epoch 45/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7609 - acc: 0.6660\n",
      "Epoch 00045: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7609 - acc: 0.6660 - val_loss: 0.7615 - val_acc: 0.6833\n",
      "Epoch 46/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 0.7904 - acc: 0.6540\n",
      "Epoch 00046: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7904 - acc: 0.6540 - val_loss: 0.7769 - val_acc: 0.6960\n",
      "Epoch 47/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9069 - acc: 0.6000\n",
      "Epoch 00047: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9069 - acc: 0.6000 - val_loss: 1.1194 - val_acc: 0.5293\n",
      "Epoch 48/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9109 - acc: 0.6020\n",
      "Epoch 00048: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.9109 - acc: 0.6020 - val_loss: 0.8921 - val_acc: 0.6460\n",
      "Epoch 49/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8289 - acc: 0.6700\n",
      "Epoch 00049: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8289 - acc: 0.6700 - val_loss: 0.8240 - val_acc: 0.6693\n",
      "Epoch 50/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8581 - acc: 0.6127\n",
      "Epoch 00050: val_acc did not improve from 0.69600\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8581 - acc: 0.6127 - val_loss: 0.8401 - val_acc: 0.6513\n",
      "Epoch 51/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8188 - acc: 0.6607\n",
      "Epoch 00051: val_acc improved from 0.69600 to 0.70267, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8188 - acc: 0.6607 - val_loss: 0.7696 - val_acc: 0.7027\n",
      "Epoch 52/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8383 - acc: 0.6247\n",
      "Epoch 00052: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8383 - acc: 0.6247 - val_loss: 0.8838 - val_acc: 0.6040\n",
      "Epoch 53/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8330 - acc: 0.6507\n",
      "Epoch 00053: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8330 - acc: 0.6507 - val_loss: 0.8177 - val_acc: 0.6773\n",
      "Epoch 54/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7955 - acc: 0.6673\n",
      "Epoch 00054: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7955 - acc: 0.6673 - val_loss: 0.8554 - val_acc: 0.6593\n",
      "Epoch 55/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7777 - acc: 0.6693\n",
      "Epoch 00055: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7777 - acc: 0.6693 - val_loss: 0.8113 - val_acc: 0.6820\n",
      "Epoch 56/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8222 - acc: 0.6607\n",
      "Epoch 00056: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8222 - acc: 0.6607 - val_loss: 0.9266 - val_acc: 0.5713\n",
      "Epoch 57/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7955 - acc: 0.6553\n",
      "Epoch 00057: val_acc did not improve from 0.70267\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7955 - acc: 0.6553 - val_loss: 0.9251 - val_acc: 0.6773\n",
      "Epoch 58/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7882 - acc: 0.6593\n",
      "Epoch 00058: val_acc improved from 0.70267 to 0.70933, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7882 - acc: 0.6593 - val_loss: 0.7449 - val_acc: 0.7093\n",
      "Epoch 59/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7680 - acc: 0.6647\n",
      "Epoch 00059: val_acc improved from 0.70933 to 0.71933, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7680 - acc: 0.6647 - val_loss: 0.7621 - val_acc: 0.7193\n",
      "Epoch 60/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7530 - acc: 0.6867\n",
      "Epoch 00060: val_acc did not improve from 0.71933\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.7530 - acc: 0.6867 - val_loss: 0.9811 - val_acc: 0.5713\n",
      "Epoch 61/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8413 - acc: 0.6252\n",
      "Epoch 00061: val_acc did not improve from 0.71933\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.8413 - acc: 0.6252 - val_loss: 0.7671 - val_acc: 0.7167\n",
      "Epoch 62/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7594 - acc: 0.6847\n",
      "Epoch 00062: val_acc improved from 0.71933 to 0.73067, saving model to best_model0.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7594 - acc: 0.6847 - val_loss: 0.7592 - val_acc: 0.7307\n",
      "Epoch 63/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7488 - acc: 0.6947\n",
      "Epoch 00063: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7488 - acc: 0.6947 - val_loss: 0.8371 - val_acc: 0.6807\n",
      "Epoch 64/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.6727\n",
      "Epoch 00064: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7800 - acc: 0.6727 - val_loss: 0.7470 - val_acc: 0.7073\n",
      "Epoch 65/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7439 - acc: 0.6900\n",
      "Epoch 00065: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7439 - acc: 0.6900 - val_loss: 0.7394 - val_acc: 0.7153\n",
      "Epoch 66/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7646 - acc: 0.6780\n",
      "Epoch 00066: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7646 - acc: 0.6780 - val_loss: 0.8380 - val_acc: 0.6480\n",
      "Epoch 67/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7629 - acc: 0.6887\n",
      "Epoch 00067: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7629 - acc: 0.6887 - val_loss: 0.7536 - val_acc: 0.6873\n",
      "Epoch 68/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7465 - acc: 0.6947\n",
      "Epoch 00068: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7465 - acc: 0.6947 - val_loss: 0.7486 - val_acc: 0.7140\n",
      "Epoch 69/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8066 - acc: 0.6520\n",
      "Epoch 00069: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 83s 2s/step - loss: 0.8066 - acc: 0.6520 - val_loss: 0.7408 - val_acc: 0.7047\n",
      "Epoch 70/70\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7517 - acc: 0.6747\n",
      "Epoch 00070: val_acc did not improve from 0.73067\n",
      "50/50 [==============================] - 82s 2s/step - loss: 0.7517 - acc: 0.6747 - val_loss: 0.8304 - val_acc: 0.6820\n"
     ]
    }
   ],
   "source": [
    "#training model\n",
    "model = Model_gender_test2()\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=50,\n",
    "                              epochs=70,\n",
    "                              validation_data = validation_generator,\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[checkpoint])\n",
    "model.save('face_gender_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93543f3-b265-4996-9dd1-9bb992a65077",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_generator(test_generator, (9296 / 20)) # ceil(num_of_test_samples / batch_size)\n",
    "target_names = ['Men', 'Woman','Baby']\n",
    "print(classification_report(test_generator.classes[:Y_pred.shape[0]],\n",
    "                            list(np.argmax(Y_pred,axis=1)), \n",
    "                            target_names=target_names))\n",
    "\n",
    "\n",
    "acc_train = history.history['acc']\n",
    "acc_val = history.history['val_acc']\n",
    "epochs = range(1,51)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, acc_val, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,51)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8110c2",
   "metadata": {},
   "source": [
    "# Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31517168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_age():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(256, (1, 1), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(512, (1, 1), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 192, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = 12, activation = 'softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40be504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_ids.age.replace(['35','(8, 12)','13','22','34','45','(27, 32)','23','55','36','(38, 42)','57'],\n",
    "                             ['(33,37)','(8,13)','(8,13)','(21,24)','(33,37)','(44,47)','(25, 32)','(21,24)','(54,59)',\n",
    "                             '(33,37)','(38, 43)','(54,59)'],\n",
    "                             inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc561720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)     2528\n",
       "(0, 2)       1255\n",
       "(8,13)       1171\n",
       "(38, 43)     1152\n",
       "(4, 6)       1082\n",
       "(15, 20)      804\n",
       "(60, 100)     444\n",
       "(48, 53)      381\n",
       "(33,37)       225\n",
       "(21,24)       120\n",
       "(44,47)        44\n",
       "(54,59)        44\n",
       "None           20\n",
       "3               7\n",
       "29              7\n",
       "58              4\n",
       "(38, 48)        3\n",
       "2               3\n",
       "42              1\n",
       "46              1\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_ids.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c57b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_ids.drop(train_images_ids[~train_images_ids.age.isin((train_images_ids.age.value_counts().head(12)).index)].index, \n",
    "                      inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6717f0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)     2528\n",
       "(0, 2)       1255\n",
       "(8,13)       1171\n",
       "(38, 43)     1152\n",
       "(4, 6)       1082\n",
       "(15, 20)      804\n",
       "(60, 100)     444\n",
       "(48, 53)      381\n",
       "(33,37)       225\n",
       "(21,24)       120\n",
       "(44,47)        44\n",
       "(54,59)        44\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_ids.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0eabac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)     2502\n",
       "(0, 2)       1233\n",
       "(38, 43)     1187\n",
       "(8,13)       1116\n",
       "(4, 6)       1058\n",
       "(15, 20)      838\n",
       "(48, 53)      444\n",
       "(60, 100)     423\n",
       "(33,37)       229\n",
       "(21,24)       125\n",
       "(54,59)        56\n",
       "(44,47)        44\n",
       "None           20\n",
       "3              11\n",
       "29              4\n",
       "(38, 48)        3\n",
       "58              1\n",
       "(8, 23)         1\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_ids.age.replace(['35','(8, 12)','13','22','34','45','(27, 32)','23','55','36','(38, 42)','57'],\n",
    "                             ['(33,37)','(8,13)','(8,13)','(21,24)','(33,37)','(44,47)','(25, 32)','(21,24)','(54,59)',\n",
    "                             '(33,37)','(38, 43)','(54,59)'],\n",
    "                             inplace = True)\n",
    "test_images_ids.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4db35458",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_ids.drop(test_images_ids[test_images_ids.age.isin((test_images_ids.age.value_counts().loc[(test_images_ids.age.value_counts().values < 30)]).index)].index, \n",
    "                     inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfbbcc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)     2502\n",
       "(0, 2)       1233\n",
       "(38, 43)     1187\n",
       "(8,13)       1116\n",
       "(4, 6)       1058\n",
       "(15, 20)      838\n",
       "(48, 53)      444\n",
       "(60, 100)     423\n",
       "(33,37)       229\n",
       "(21,24)       125\n",
       "(54,59)        56\n",
       "(44,47)        44\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_ids.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a427a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9250 validated image filenames belonging to 12 classes.\n",
      "Found 9255 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    train_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='age',\n",
    "                    directory=train_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical'\n",
    "                )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                    test_images_ids,\n",
    "                    x_col='original_image',\n",
    "                    y_col='age',\n",
    "                    directory=test_faces,\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=100,\n",
    "                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ec4d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_78 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_77 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_78 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 17, 17, 256)       33024     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_79 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 8, 8, 512)         131584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_80 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 192)               1573056   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 1,860,140\n",
      "Trainable params: 1,860,140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "93/93 [==============================] - 326s 4s/step - loss: 2.3299 - acc: 0.1730 - val_loss: 2.1979 - val_acc: 0.2703\n",
      "Epoch 2/50\n",
      "93/93 [==============================] - 328s 4s/step - loss: 2.2408 - acc: 0.2164 - val_loss: 2.1632 - val_acc: 0.2703\n",
      "Epoch 3/50\n",
      "86/93 [==========================>...] - ETA: 13s - loss: 2.1922 - acc: 0.2337"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-5f9ca22da3dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_age\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_age\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history_age = model_age.fit(train_generator,\n\u001b[0m\u001b[0;32m      3\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m93\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_age = Model_age()\n",
    "history_age = model_age.fit(train_generator,\n",
    "                              steps_per_epoch=93,\n",
    "                              epochs=50,\n",
    "                              validation_data = test_generator,\n",
    "                              validation_steps=93)\n",
    "model_age.save('face_age_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09767c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
